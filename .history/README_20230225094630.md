# BTmPG
Code for paper [Visual Information Guided Zero-Shot Paraphrase Generation](https://aclanthology.org/2022.coling-1.568/) by Zhe Lin, Xiaojun Wan. This paper is accepted by Findings of COLING'22. Please contact me at [linzhe@pku.edu.cn](mailto:linzhe@pku.edu.cn) for any question.

# Dependencies
```
pytorch 1.4
StanfordCoreNLP
Pillow
numpy
Transformer
subword-nmt
```

<img src="https://github.com/L-Zhe/ViPG/blob/main/img/model.jpg?raw=true" width = "800" alt="overview" align=center />

# Train a new model
## Step 1: Preprocess

### Image

You should first use ```gerate_img_feature.py``` to generate image feature vector. We note the output file of image feature vector as ```image_feature```

### Text
You should first leverage ```POS.py``` to transform a text to the following format, and you will get the text file ```*.pos```.
<img src="https://github.com/L-Zhe/ViPG/blob/main/img/ner.jpg?raw=true" width = "800" alt="overview" align=center />


Then you can use the following command to apply byte pair encoding to word segmentation:

```
subword-nmt learn-bpe -s 32000 < *.pos > *.pos.code
subword-nmt apply-bpe -c *.pos.code < *.pos > *.pos.bpe
```

Next, you should generate vocabulary of corpora as follow:

```
python createVocab.py --file *.pos.bpe\
                      --lower --save_path ./vocab.share  --min_freq 1
```

Finally, you can employ the folloing command to generate the training data file:

```
python preprocess.py --sent_file *.pos.bpe \
                     --img_file image_file.img \
                     --vocab vocab.share \
                     --save_file train.data
```
```image_file.img``` is a text file, each line of it is an image file name which orresponds to the training file ```.pos.bpe``` .

## Step 2: Train Model

```
python train.py --cuda_num 2\
                --share_embed \
                --vocab ./vocab.share \
                --file ./train.data\
                --img_path ./image_feature/ \
                --checkpoint_path ./model \
                --checkpoint_n_epoch 1 \
                --grad_accum 1 \
                --max_tokens 5000 \
                --max_batch_size 256 \
                --discard_invalid_data
```
